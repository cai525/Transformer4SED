include:
  base_path: ./config/pmam/finetune1.yaml
  keys: [feature, dataset, synth_dataset, class_loss]

generals:
  warn: False                          # show warnings
  savepsds: True                       # save psds data
  test_on_public_eval: False           # change test dataset to DESED Reak public_eval (default dataset is DESED Real Validation)
  load_from_existed_path: True         # Load model in existed path instead of creating a new one if true
  finetune_mlm: False
  test_only: False                     # perform test without training, for the model saved in save_folder
  validation_interval: 1
  log_level: INFO

training:
  batch_size: [4, 2, 6, 6]  # strong, synthetic, weak, unlabeled
  batch_size_val: 24
  num_workers: 6
  seed: 21
  clip_grad: True
  # =============================
  cons_scheduler_name: Sigmoid
  self_loss_warmup: 15
  scheduler:
    scheduler_name: ExponentialDown
    n_epochs: 30                        #>> number of epochs to run
    n_epochs_cut: 15                  #>> number of epochs used for exponential warmup
    exponent: -1.5
    lr_warmup_rate: 0.1
    lr_warmup_epochs: 1
  # =============================
  ema_factor: 0.999                    # ema factor for teacher model used in mean teacher model
  w_weak: 0.5                          # weight for weak classification cost
  w_cons_max: 40                      #>> max weight used for consistency loss
  w_cons_min: 0
  w_weak_cons: 0.5                       # max weight for weak classification cost
  w_AT: 2
  weak_mask: True
  median_window: [ 5,20, 5, 5, 5,20,20,20, 5,20]
  #data augmentations
  transform:                           # hyperparameters for data augmentations that do not alter the label information.
    n_transform: 2                     # 0: no augmentation below is applied. 1: same augmentation below is applied on student/teacher model input. 2: different augmentations below is applied on student/teacher model input.
    choice: [ 1, 0, 0, 1]              # apply the chosen data augmentations: [ FilterAugment, freq_mask, add_noise, frequency disortion]
    # arguments of FilterAugument
    filter_db_range: [-26, 26]         # db range of FilterAugment to be applied on each band
    filter_bands: [ 2, 5 ]             # range of frequency band number in FilterAugment
    filter_minimum_bandwidth: 4
    filter_type: step

PaSST_CNN:
  init_kwargs:
    passt_sed_param:
      passt_feature_layer: 10                     # from which layer in AST to get patch embeddings
      f_pool: "attention"             # frequency-wise information pooling type
      decode_ratio: 10                     # decode ratio in local gru decoder
      at_adapter: True
      decoder: "transformerXL"
      decoder_layer_num: 3
      decoder_pos_emd_len: 1000
      decoder_dim: 384
      mlm: False
    cnn_param:
      n_in_channel: 1
      activation: cg
      conv_dropout: 0.5
      kernel_size: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
      padding: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
      stride: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
      nb_filters: [ 16, 16, 32, 32, 64, 64, 128, 128, 256, 384]
      pooling: [ [ 2, 2 ], [1, 1], [ 2, 2 ],[1, 1], [ 1, 2 ], [ 1, 2 ], [ 1, 2 ], [ 1, 2 ], [ 1, 2 ], [1, 1] ]

  train_stu_kwargs:
    # windows parameters
    encoder_win: False    # constant
    win_param: [512, 49]
    mix_rate: 0.5 
    # temperature parameters
    temp_w: 1
  
  train_tch_kwargs:
    # windows parameters
    encoder_win: True
    win_param: [512, 49]
    mix_rate: 0.5 
    # temperature parameters
    temp_w: 1
  
  val_kwargs:
    # windows parameters
    encoder_win: True
    win_param: [512, 31]
    mix_rate: 0.5 
    # temperature parameters
    temp_w: 0.5

opt: 
    param_groups: 
      cnn:
        lr: 1.5e-4
        weight_decay: 1.0e-4
      passt:
        lr: 7.5e-6
        weight_decay: 1.0e-4
        freeze_layer: 0
        step_lr: 4
      decoder: 
        lr: 1.5e-4
        weight_decay: 1.0e-4
      head:
        lr: 2.0e-4
        weight_decay: 1.0e-4
      